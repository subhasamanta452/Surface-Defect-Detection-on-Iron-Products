{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24112b23-c36d-4be2-9ca3-e6ba870f88f3",
      "metadata": {
        "id": "24112b23-c36d-4be2-9ca3-e6ba870f88f3",
        "outputId": "42f31f45-ffb4-41b8-cc50-b161f4985727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up Perceptual loss...\n",
            "Loading model from: /home/dnn4/pythonCodeArea/Ashish/Final_Dissertation/FastGAn/FastGAN-pytorch/lpips/weights/v0.1/vgg.pth\n",
            "...[net-lin [vgg]] initialized\n",
            "...Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import utils as vutils\n",
        "import torchvision.models as models\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import weights_init, Discriminator, Generator\n",
        "from operation import copy_G_params, load_params, get_dir\n",
        "from operation import ImageFolder, InfiniteSamplerWrapper\n",
        "from diffaug import DiffAugment\n",
        "policy = 'color,translation'\n",
        "import lpips\n",
        "percept = lpips.PerceptualLoss(model='net-lin', net='vgg', use_gpu=True)\n",
        "\n",
        "from helper_function import get_covariance, frechet_distance\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def crop_image_by_part(image, part):\n",
        "    hw = image.shape[2]//2\n",
        "    if part==0:\n",
        "        return image[:,:,:hw,:hw]\n",
        "    if part==1:\n",
        "        return image[:,:,:hw,hw:]\n",
        "    if part==2:\n",
        "        return image[:,:,hw:,:hw]\n",
        "    if part==3:\n",
        "        return image[:,:,hw:,hw:]\n",
        "\n",
        "def train_d(net, data, label=\"real\"):\n",
        "    \"\"\"Train function of discriminator\"\"\"\n",
        "    if label==\"real\":\n",
        "        part = random.randint(0, 3)\n",
        "        pred, [rec_all, rec_small, rec_part] = net(data, label, part=part)\n",
        "        err = F.relu(  torch.rand_like(pred) * 0.2 + 0.8 -  pred).mean() + \\\n",
        "            percept( rec_all, F.interpolate(data, rec_all.shape[2]) ).sum() +\\\n",
        "            percept( rec_small, F.interpolate(data, rec_small.shape[2]) ).sum() +\\\n",
        "            percept( rec_part, F.interpolate(crop_image_by_part(data, part), rec_part.shape[2]) ).sum()\n",
        "        err.backward()\n",
        "        return pred.mean().item(), rec_all, rec_small, rec_part\n",
        "    else:\n",
        "        pred = net(data, label)\n",
        "        err = F.relu( torch.rand_like(pred) * 0.2 + 0.8 + pred).mean()\n",
        "        err.backward()\n",
        "        return pred.mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df7a9c1-c13a-4acb-a203-de1f5e313bf5",
      "metadata": {
        "id": "7df7a9c1-c13a-4acb-a203-de1f5e313bf5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class CLAHETransform(object):\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        lab_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2LAB)\n",
        "        l, a, b = cv2.split(lab_img)\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "        clahe_img = clahe.apply(l)\n",
        "        l = clahe.apply(l)\n",
        "        img_lab = cv2.merge((l, a, b))\n",
        "        img_np = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
        "        img = Image.fromarray(img_np)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c822afb1-a09e-43e2-9933-e40e214f5fb4",
      "metadata": {
        "id": "c822afb1-a09e-43e2-9933-e40e214f5fb4"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    data_root = args.path\n",
        "    total_iterations = args.iter\n",
        "    checkpoint = args.ckpt\n",
        "    batch_size = args.batch_size\n",
        "    im_size = args.im_size\n",
        "    ndf = 64\n",
        "    ngf = 64\n",
        "    nz = 256\n",
        "    nlr = 0.0002\n",
        "    nbeta1 = 0.5\n",
        "    use_cuda = True\n",
        "    multi_gpu = True\n",
        "    # n_epochs = args.n_epochs\n",
        "    dataloader_workers = args.workers\n",
        "    current_iteration = args.start_iter\n",
        "    save_interval = args.save_interval\n",
        "    saved_model_folder, saved_image_folder = get_dir(args)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "\n",
        "    transform_list = [\n",
        "            transforms.Resize((int(im_size),int(im_size))),\n",
        "            CLAHETransform(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ]\n",
        "    trans = transforms.Compose(transform_list)\n",
        "\n",
        "    if 'lmdb' in data_root:\n",
        "        from operation import MultiResolutionDataset\n",
        "        dataset = MultiResolutionDataset(data_root, trans, 1024)\n",
        "    else:\n",
        "        dataset = ImageFolder(root=data_root, transform=trans)\n",
        "\n",
        "\n",
        "    dataloader = iter(DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                      sampler=InfiniteSamplerWrapper(dataset), num_workers=dataloader_workers, pin_memory=True))\n",
        "    '''\n",
        "    loader = MultiEpochsDataLoader(dataset, batch_size=batch_size,\n",
        "                               shuffle=True, num_workers=dataloader_workers,\n",
        "                               pin_memory=True)\n",
        "    dataloader = CudaDataLoader(loader, 'cuda')\n",
        "    '''\n",
        "\n",
        "\n",
        "    #from model_s import Generator, Discriminator\n",
        "    netG = Generator(ngf=ngf, nz=nz, im_size=im_size)\n",
        "    netG.apply(weights_init)\n",
        "\n",
        "    netD = Discriminator(ndf=ndf, im_size=im_size)\n",
        "    netD.apply(weights_init)\n",
        "\n",
        "    netG.to(device)\n",
        "    netD.to(device)\n",
        "\n",
        "    avg_param_G = copy_G_params(netG)\n",
        "\n",
        "    fixed_noise = torch.FloatTensor(8, nz).normal_(0, 1).to(device)\n",
        "\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
        "\n",
        "    if checkpoint != 'None':\n",
        "        ckpt = torch.load(checkpoint)\n",
        "        netG.load_state_dict({k.replace('module.', ''): v for k, v in ckpt['g'].items()})\n",
        "        netD.load_state_dict({k.replace('module.', ''): v for k, v in ckpt['d'].items()})\n",
        "        avg_param_G = ckpt['g_ema']\n",
        "        optimizerG.load_state_dict(ckpt['opt_g'])\n",
        "        optimizerD.load_state_dict(ckpt['opt_d'])\n",
        "        current_iteration = int(checkpoint.split('_')[-1].split('.')[0])\n",
        "        # current_iteration = 50000\n",
        "        print(\"checkpoint loaded successfully\")\n",
        "        # del ckpt\n",
        "\n",
        "    if multi_gpu:\n",
        "        netG = nn.DataParallel(netG.to(device))\n",
        "        netD = nn.DataParallel(netD.to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for iteration in tqdm(range(current_iteration, total_iterations+1)):\n",
        "        real_image = next(dataloader)\n",
        "        real_image = real_image.to(device)\n",
        "        current_batch_size = real_image.size(0)\n",
        "        noise = torch.Tensor(current_batch_size, nz).normal_(0, 1).to(device)\n",
        "\n",
        "        fake_images = netG(noise)\n",
        "\n",
        "        real_image = DiffAugment(real_image, policy=policy)\n",
        "        fake_images = [DiffAugment(fake, policy=policy) for fake in fake_images]\n",
        "\n",
        "        ## 2. train Discriminator\n",
        "        netD.zero_grad()\n",
        "\n",
        "        err_dr, rec_img_all, rec_img_small, rec_img_part = train_d(netD, real_image, label=\"real\")\n",
        "        train_d(netD, [fi.detach() for fi in fake_images], label=\"fake\")\n",
        "        optimizerD.step()\n",
        "\n",
        "        ## 3. train Generator\n",
        "        netG.zero_grad()\n",
        "        pred_g = netD(fake_images, \"fake\")\n",
        "        err_g = -pred_g.mean()\n",
        "\n",
        "        err_g.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        for p, avg_p in zip(netG.parameters(), avg_param_G):\n",
        "            avg_p.mul_(0.999).add_(0.001 * p.data)\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"GAN: loss d: %.5f    loss g: %.5f\"%(err_dr, -err_g.item()))\n",
        "\n",
        "        if iteration % (save_interval) == 0:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            with torch.no_grad():\n",
        "                vutils.save_image(netG(fixed_noise)[0].add(1).mul(0.5), saved_image_folder+'/%d.jpg'%iteration, nrow=4)\n",
        "                vutils.save_image( torch.cat([\n",
        "                        F.interpolate(real_image, 128),\n",
        "                        rec_img_all, rec_img_small,\n",
        "                        rec_img_part]).add(1).mul(0.5), saved_image_folder+'/rec_%d.jpg'%iteration )\n",
        "            load_params(netG, backup_para)\n",
        "\n",
        "        if iteration % (save_interval) == 0 or iteration == total_iterations:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            torch.save({'g':netG.state_dict(),'d':netD.state_dict()}, saved_model_folder+'/%d.pth'%iteration)\n",
        "            load_params(netG, backup_para)\n",
        "            torch.save({'g':netG.state_dict(),\n",
        "                        'd':netD.state_dict(),\n",
        "                        'g_ema': avg_param_G,\n",
        "                        'opt_g': optimizerG.state_dict(),\n",
        "                        'opt_d': optimizerD.state_dict()}, saved_model_folder+'/all_%d.pth'%iteration)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b228d691-2f10-4520-b832-bddb1be97f7b",
      "metadata": {
        "id": "b228d691-2f10-4520-b832-bddb1be97f7b"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser(description='region gan')\n",
        "\n",
        "#     parser.add_argument('--path', type=str, default='../lmdbs/art_landscape_1k', help='path of resource dataset, should be a folder that has one or many sub image folders inside')\n",
        "#     parser.add_argument('--output_path', type=str, default='./', help='Output path for the train results')\n",
        "#     parser.add_argument('--cuda', type=int, default=0, help='index of gpu to use')\n",
        "#     parser.add_argument('--name', type=str, default='test1', help='experiment name')\n",
        "#     parser.add_argument('--iter', type=int, default=50000, help='number of iterations')\n",
        "#     parser.add_argument('--start_iter', type=int, default=0, help='the iteration to start training')\n",
        "#     parser.add_argument('--batch_size', type=int, default=8, help='mini batch number of images')\n",
        "#     parser.add_argument('--im_size', type=int, default=1024, help='image resolution')\n",
        "#     parser.add_argument('--ckpt', type=str, default='None', help='checkpoint weight path if have one')\n",
        "#     parser.add_argument('--workers', type=int, default=2, help='number of workers for dataloader')\n",
        "#     parser.add_argument('--save_interval', type=int, default=100, help='number of iterations to save model')\n",
        "\n",
        "class argument:\n",
        "    def __init__(self, path='./', output_path='./', cuda=0, name='result', iter=50000,\n",
        "                 start_iter=0, batch_size=8, im_size=512, ckpt='None', workers=2, save_interval=50):\n",
        "        self.path = path\n",
        "        self.output_path = output_path\n",
        "        self.cuda = cuda\n",
        "        self.name = name\n",
        "        self.iter = iter\n",
        "        self.start_iter = start_iter\n",
        "        self.batch_size = batch_size\n",
        "        self.im_size = im_size\n",
        "        self.ckpt = ckpt\n",
        "        self.workers = workers\n",
        "        self.save_interval = save_interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fdb726-2ee0-4c8f-8ed7-85916b1ca99c",
      "metadata": {
        "id": "47fdb726-2ee0-4c8f-8ed7-85916b1ca99c"
      },
      "outputs": [],
      "source": [
        "arg = argument()\n",
        "arg.path = \"../../DATA/Classification_Dataset/original/Train/CR Scale RIS\"\n",
        "arg.ckpt = \"train_results/test1/models/all_50000.pth\"\n",
        "arg.iter = 100000\n",
        "arg.name = \"HR Sliver NMI\"\n",
        "train(arg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6e56fa-e7ec-4892-93c0-296bb3008d2f",
      "metadata": {
        "id": "df6e56fa-e7ec-4892-93c0-296bb3008d2f"
      },
      "source": [
        "# calculation of FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b5059f",
      "metadata": {
        "id": "c9b5059f"
      },
      "outputs": [],
      "source": [
        "arg = argument()\n",
        "arg.iter = 50000\n",
        "arg.name = \"HR Scab Patch\"\n",
        "arg.path = \"../../DATA/04_Mar_24/HR Scab Patch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b800543d",
      "metadata": {
        "id": "b800543d"
      },
      "outputs": [],
      "source": [
        "def FD(args):\n",
        "    data_root = args.path\n",
        "    total_iterations = args.iter\n",
        "    checkpoint = args.ckpt\n",
        "    batch_size = args.batch_size\n",
        "    im_size = args.im_size\n",
        "    ndf = 64\n",
        "    ngf = 64\n",
        "    nz = 256\n",
        "    nlr = 0.0002\n",
        "    nbeta1 = 0.5\n",
        "    use_cuda = True\n",
        "    multi_gpu = True\n",
        "    dataloader_workers = args.workers\n",
        "    current_iteration = args.start_iter\n",
        "    save_interval = args.save_interval\n",
        "    saved_model_folder, saved_image_folder = get_dir(args)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "\n",
        "    transform_list = [\n",
        "            transforms.Resize((int(im_size),int(im_size))),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ]\n",
        "    trans = transforms.Compose(transform_list)\n",
        "\n",
        "    if 'lmdb' in data_root:\n",
        "        from operation import MultiResolutionDataset\n",
        "        dataset = MultiResolutionDataset(data_root, trans, 1024)\n",
        "    else:\n",
        "        dataset = ImageFolder(root=data_root, transform=trans)\n",
        "\n",
        "\n",
        "    print(\"size of dataset\",len(dataset))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
        "    # dataloader = iter(DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "    #                   sampler=InfiniteSamplerWrapper(dataset), num_workers=dataloader_workers, pin_memory=True))\n",
        "\n",
        "\n",
        "\n",
        "    inception_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "    inception_model.fc = torch.nn.Identity()\n",
        "    inception_model.to(device)\n",
        "\n",
        "    real_features_list = []\n",
        "    n_samples = len(dataset) # The total number of samples\n",
        "\n",
        "    inception_model = inception_model.eval()\n",
        "    with torch.no_grad(): # You don't need to calculate gradients here, so you do this to save memory\n",
        "        try:\n",
        "            for real_example in tqdm(dataloader, total=n_samples // batch_size): # Go by batch\n",
        "                real_samples = real_example\n",
        "                real_features = inception_model(real_samples.to(device)).detach().to('cpu') # Move features to CPU\n",
        "                real_features_list.append(real_features)\n",
        "        except:\n",
        "            print(\"Error in real loop\")\n",
        "\n",
        "\n",
        "    real_features_all = torch.cat(real_features_list)\n",
        "    mu_real = real_features_all.mean(0)\n",
        "    sigma_real = get_covariance(real_features_all)\n",
        "\n",
        "    FID = []\n",
        "    epochs = [i for i in range(77500,100001,500)]\n",
        "    net_ig = Generator( ngf=64, nz= nz, nc=3, im_size=args.im_size)\n",
        "    net_ig.to(device)\n",
        "    for epoch in epochs:\n",
        "        fake_features_list = []\n",
        "#################################################################################################\n",
        "\n",
        "        ckpt = f\"train_results/{args.name}/models/{epoch}.pth\"\n",
        "        checkpoint = torch.load(ckpt, map_location=lambda a,b: a)\n",
        "        # Remove prefix `module`.\n",
        "        checkpoint['g'] = {k.replace('module.', ''): v for k, v in checkpoint['g'].items()}\n",
        "        net_ig.load_state_dict(checkpoint['g'])\n",
        "        #load_params(net_ig, checkpoint['g_ema'])\n",
        "        print('load checkpoint success, epoch %d'%epoch)\n",
        "\n",
        "        net_ig.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                for real_example in tqdm(dataloader, total=n_samples // batch_size): # Go by batch\n",
        "                    fake_examples = torch.Tensor(len(real_example), nz).normal_(0, 1).to(device)\n",
        "                    fake_samples = net_ig(fake_examples)\n",
        "                    fake_features = inception_model(fake_samples[0]).detach().to('cpu')\n",
        "                    fake_features_list.append(fake_features)\n",
        "            except:\n",
        "                print(\"Error in fake loop\")\n",
        "        fake_features_all = torch.cat(fake_features_list)\n",
        "        mu_fake = fake_features_all.mean(0)\n",
        "        sigma_fake = get_covariance(fake_features_all)\n",
        "        fid = frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item()\n",
        "        print(fid)\n",
        "        FID.append(fid)\n",
        "        # with torch.no_grad():\n",
        "        #     print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())\n",
        "    return FID,epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd84ef1",
      "metadata": {
        "id": "3cd84ef1",
        "outputId": "afa64dbd-f419-4a8d-9f6d-3de3d71e7f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of dataset 56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 20.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint success, epoch 77500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.294979095458984\n",
            "load checkpoint success, epoch 78000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61.90937805175781\n",
            "load checkpoint success, epoch 78500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.44194412231445\n",
            "load checkpoint success, epoch 79000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.55289840698242\n",
            "load checkpoint success, epoch 79500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.049556732177734\n",
            "load checkpoint success, epoch 80000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50.32158279418945\n",
            "load checkpoint success, epoch 80500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.59404754638672\n",
            "load checkpoint success, epoch 81000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.961158752441406\n",
            "load checkpoint success, epoch 81500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.41719436645508\n",
            "load checkpoint success, epoch 82000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58.539493560791016\n",
            "load checkpoint success, epoch 82500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.01541519165039\n",
            "load checkpoint success, epoch 83000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.60749816894531\n",
            "load checkpoint success, epoch 83500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.236480712890625\n",
            "load checkpoint success, epoch 84000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52.32200622558594\n",
            "load checkpoint success, epoch 84500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.532615661621094\n",
            "load checkpoint success, epoch 85000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51.14384841918945\n",
            "load checkpoint success, epoch 85500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.35237503051758\n",
            "load checkpoint success, epoch 86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.5759162902832\n",
            "load checkpoint success, epoch 86500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48.90548324584961\n",
            "load checkpoint success, epoch 87000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49.619327545166016\n",
            "load checkpoint success, epoch 87500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.35722732543945\n",
            "load checkpoint success, epoch 88000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.20051574707031\n",
            "load checkpoint success, epoch 88500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.45621109008789\n",
            "load checkpoint success, epoch 89000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.36295700073242\n",
            "load checkpoint success, epoch 89500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59.099971771240234\n",
            "load checkpoint success, epoch 90000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.865234375\n",
            "load checkpoint success, epoch 90500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.0011100769043\n",
            "load checkpoint success, epoch 91000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.8021240234375\n",
            "load checkpoint success, epoch 91500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51.938575744628906\n",
            "load checkpoint success, epoch 92000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.81195068359375\n",
            "load checkpoint success, epoch 92500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51.88690185546875\n",
            "load checkpoint success, epoch 93000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.61791229248047\n",
            "load checkpoint success, epoch 93500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.71633529663086\n",
            "load checkpoint success, epoch 94000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56.821163177490234\n",
            "load checkpoint success, epoch 94500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63.63819122314453\n",
            "load checkpoint success, epoch 95000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.48065948486328\n",
            "load checkpoint success, epoch 95500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.95498275756836\n",
            "load checkpoint success, epoch 96000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54.77057647705078\n",
            "load checkpoint success, epoch 96500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.1870231628418\n",
            "load checkpoint success, epoch 97000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.89139938354492\n",
            "load checkpoint success, epoch 97500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.665618896484375\n",
            "load checkpoint success, epoch 98000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.38545227050781\n",
            "load checkpoint success, epoch 98500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58.847618103027344\n",
            "load checkpoint success, epoch 99000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.35979080200195\n",
            "load checkpoint success, epoch 99500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60.85419845581055\n",
            "load checkpoint success, epoch 100000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55.826568603515625\n"
          ]
        }
      ],
      "source": [
        "y,z = FD(arg)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}